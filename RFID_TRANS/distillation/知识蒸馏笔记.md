## 知识蒸馏笔记

1. 概念：知识迁移 可以防止过拟合 无限大的数据集输入 -> 教师模型传授“知识”给学生模型

   

2. 作用：模型压缩 在算力有限的情况下不能联网时，在移动终端部署，需要短小而精悍 

   

3. 举例：bert已经达到340M大小

   

4. 相关方向：权值量化 剪枝操作 注意力迁移

   

5. 考虑的因素：参数量 计算量 内存访问次数 耗时 cuda加速 注意力 对抗学习

   #### 轻量化模型的方法

   1. 压缩模型
   2. 训练轻量模型
   3. 加速卷积
   4. 硬件部署

6. hard targets与soft targets的理解：hard是硬的 真实的情况！ soft是软的 通过分类器输出的 代表了不同类之间一定的关系！

   因此soft输出中包含了更多的知识和信息，有多像，相对的概率，这就是它作为教师的前提基础！

   

7. 蒸馏的理解：蒸馏温度T

   T = 1，原生的softmax

   T = 10，可以更明显暴露出非正确类别的相对信息，放大差异

   T越小越两级分化，相对的差异越大！T越大越平均，相对的差异就越小！

   

8. 学习过程：

   1. 定义教师模型-训练教师模型

   2. 定义学生模型-知识蒸馏训练学生模型（学生模型可以半训练）

   3. 损失兼顾温度为T时与教师模型接近，又要温度为1时与真实值接近

   4. all loss = a * hard loss + (1-a) * soft loss

      

9. label smothing介绍：除了真实的预测值 其他全部设置为小常数 可能会丢失一些信息

   

10. 知识蒸馏的发展趋势：

    1. 教学相长：学生与老师互相成长

    2. 助教角色引入，多个老师

    3. 知识的表示（中间层来蒸馏 response feature relation） 数据集蒸馏 对比学习

    4. 多模态 知识图谱 大模型蒸馏 

       

11. 知识蒸馏论文精读（略）

    

12. 代码实战-思路

idea1：使用gru+cnn 实验提升强度

idea2：使用小变压器 实验提升

idea3：使用别的模型 实验

idea4：更换回归的损失函数-回归蒸馏

13. 